{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 3: Data Cleaning - Guided Exercise\n",
    "\n",
    "**Duration:** ~30 minutes  \n",
    "**Dataset:** Education Statistics from the Colombian Ministry of Education (datos.gov.co)  \n",
    "**Rows:** 482 (dirty) | **Columns:** 37  \n",
    "**Goal:** Clean this dataset by fixing 5 types of data quality issues  \n",
    "\n",
    "---\n",
    "\n",
    "**How to use this notebook:**\n",
    "\n",
    "1. Read each explanation cell carefully\n",
    "2. Run the code cell below it (Shift + Enter)\n",
    "3. Read the \"What just happened\" follow-up\n",
    "4. Answer the questions or complete the \"Your Turn\" exercises\n",
    "\n",
    "Think of this entire process as doing laundry: we need to sort, wash, dry, and fold our data before it is ready to wear."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "We start by importing our tools and loading the CSV file. We also make a copy of the original data so we can compare before and after at the end.\n",
    "\n",
    "**Run the cell below** to load the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset loaded: 482 rows, 37 columns\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "df = pd.read_csv('data/educacion_estadisticas.csv')\n",
    "\n",
    "# Keep a copy of the original so we can compare at the end\n",
    "df_original = df.copy()\n",
    "\n",
    "print(f\"Dataset loaded: {df.shape[0]} rows, {df.shape[1]} columns\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset has 482 rows and 37 columns. Each row represents one Colombian department in one year (2011-2024), with education indicators like enrollment rates, dropout rates, and coverage.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 1: Data Inspection\n",
    "\n",
    "Before cleaning anything, we need to understand what we are working with. Think of this as **opening the laundry bag** and checking what is inside before turning on the washing machine. You would not throw everything in without looking first.\n",
    "\n",
    "This is the **inspection ritual**: a set of commands you run at the start of every data project."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ffa8fac",
   "metadata": {},
   "source": [
    "### 1.1 Shape and Column Names\n",
    "\n",
    "We use `df.shape` to see how many rows and columns exist, and `df.columns.tolist()` to see the names of all columns.\n",
    "\n",
    "**Run the cell below.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rows: 482\n",
      "Columns: 37\n",
      "\n",
      "Column names:\n",
      "['ano', 'c_digo_departamento', 'departamento', 'poblacion_5_16', 'tasa_matriculacion_5_16', 'cobertura_neta', 'cobertura_neta_transicion', 'cobertura_neta_primaria', 'cobertura_neta_secundaria', 'cobertura_neta_media', 'cobertura_bruta', 'cobertura_bruta_transicion', 'cobertura_bruta_primaria', 'cobertura_bruta_secundaria', 'cobertura_bruta_media', 'tamano_promedio_grupo', 'sedes_conectadas_a_internet', 'desercion', 'desercion_transicion', 'desercion_primaria', 'desercion_secundaria', 'desercion_media', 'aprobacion', 'aprobacion_transicion', 'aprobacion_primaria', 'aprobacion_secundaria', 'aprobacion_media', 'reprobacion', 'reprobacion_transicion', 'reprobacion_primaria', 'reprobacion_secundaria', 'reprobacion_media', 'repitencia', 'repitencia_transicion', 'repitencia_primaria', 'repitencia_secundaria', 'repitencia_media']\n"
     ]
    }
   ],
   "source": [
    "print(f\"Rows: {df.shape[0]}\")\n",
    "print(f\"Columns: {df.shape[1]}\")\n",
    "print()\n",
    "print(\"Column names:\")\n",
    "print(df.columns.tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**What to notice:** The column names are in Spanish (as expected from a Colombian government dataset). Key columns include:\n",
    "- `ano` = year\n",
    "- `departamento` = department (geographic region)\n",
    "- `poblacion_5_16` = population aged 5-16\n",
    "- `desercion` = dropout rate\n",
    "- `cobertura_neta` = net enrollment coverage\n",
    "- `aprobacion` / `reprobacion` = approval / failure rates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 First Rows\n",
    "\n",
    "Looking at the actual data helps us spot problems that column names alone cannot reveal. `df.head()` shows the first 5 rows.\n",
    "\n",
    "**Run the cell below.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ano</th>\n",
       "      <th>c_digo_departamento</th>\n",
       "      <th>departamento</th>\n",
       "      <th>poblacion_5_16</th>\n",
       "      <th>tasa_matriculacion_5_16</th>\n",
       "      <th>cobertura_neta</th>\n",
       "      <th>cobertura_neta_transicion</th>\n",
       "      <th>cobertura_neta_primaria</th>\n",
       "      <th>cobertura_neta_secundaria</th>\n",
       "      <th>cobertura_neta_media</th>\n",
       "      <th>...</th>\n",
       "      <th>reprobacion</th>\n",
       "      <th>reprobacion_transicion</th>\n",
       "      <th>reprobacion_primaria</th>\n",
       "      <th>reprobacion_secundaria</th>\n",
       "      <th>reprobacion_media</th>\n",
       "      <th>repitencia</th>\n",
       "      <th>repitencia_transicion</th>\n",
       "      <th>repitencia_primaria</th>\n",
       "      <th>repitencia_secundaria</th>\n",
       "      <th>repitencia_media</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2023.0</td>\n",
       "      <td>68</td>\n",
       "      <td>Santander</td>\n",
       "      <td>405256</td>\n",
       "      <td>95.80</td>\n",
       "      <td>95.64</td>\n",
       "      <td>71.27</td>\n",
       "      <td>92.98</td>\n",
       "      <td>83.40</td>\n",
       "      <td>56.02</td>\n",
       "      <td>...</td>\n",
       "      <td>7.65</td>\n",
       "      <td>2.95</td>\n",
       "      <td>91.82</td>\n",
       "      <td>12.95</td>\n",
       "      <td>6.92</td>\n",
       "      <td>7.99</td>\n",
       "      <td>2.95</td>\n",
       "      <td>7.32</td>\n",
       "      <td>11.34</td>\n",
       "      <td>4.07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2024.0</td>\n",
       "      <td>86</td>\n",
       "      <td>Putumayo</td>\n",
       "      <td>81100</td>\n",
       "      <td>80.03</td>\n",
       "      <td>80.01</td>\n",
       "      <td>50.77</td>\n",
       "      <td>78.12</td>\n",
       "      <td>66.49</td>\n",
       "      <td>38.12</td>\n",
       "      <td>...</td>\n",
       "      <td>6.07</td>\n",
       "      <td>3.02</td>\n",
       "      <td>90.68</td>\n",
       "      <td>10.04</td>\n",
       "      <td>4.82</td>\n",
       "      <td>8.50</td>\n",
       "      <td>3.02</td>\n",
       "      <td>7.56</td>\n",
       "      <td>12.33</td>\n",
       "      <td>4.57</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2016.0</td>\n",
       "      <td>52</td>\n",
       "      <td>Nariño</td>\n",
       "      <td>394,574</td>\n",
       "      <td>71.10</td>\n",
       "      <td>NaN</td>\n",
       "      <td>38.56</td>\n",
       "      <td>68.32</td>\n",
       "      <td>56.56</td>\n",
       "      <td>29.80</td>\n",
       "      <td>...</td>\n",
       "      <td>1.89</td>\n",
       "      <td>0.04</td>\n",
       "      <td>97.71</td>\n",
       "      <td>3.07</td>\n",
       "      <td>1.92</td>\n",
       "      <td>2.64</td>\n",
       "      <td>0.04</td>\n",
       "      <td>2.36</td>\n",
       "      <td>3.79</td>\n",
       "      <td>1.34</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2024.0</td>\n",
       "      <td>50</td>\n",
       "      <td>Meta</td>\n",
       "      <td>221911</td>\n",
       "      <td>90.34</td>\n",
       "      <td>90.29</td>\n",
       "      <td>65.69</td>\n",
       "      <td>89.17</td>\n",
       "      <td>76.00</td>\n",
       "      <td>48.10</td>\n",
       "      <td>...</td>\n",
       "      <td>6.84</td>\n",
       "      <td>2.85</td>\n",
       "      <td>91.18</td>\n",
       "      <td>11.36</td>\n",
       "      <td>5.61</td>\n",
       "      <td>9.27</td>\n",
       "      <td>2.85</td>\n",
       "      <td>9.27</td>\n",
       "      <td>12.53</td>\n",
       "      <td>3.49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2020.0</td>\n",
       "      <td>5</td>\n",
       "      <td>antioquia</td>\n",
       "      <td>1151149</td>\n",
       "      <td>93.59</td>\n",
       "      <td>93.43</td>\n",
       "      <td>70.25</td>\n",
       "      <td>88.26</td>\n",
       "      <td>83.38</td>\n",
       "      <td>47.45</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.79</td>\n",
       "      <td>92.19</td>\n",
       "      <td>11.46</td>\n",
       "      <td>7.12</td>\n",
       "      <td>6.48</td>\n",
       "      <td>0.79</td>\n",
       "      <td>5.92</td>\n",
       "      <td>9.37</td>\n",
       "      <td>3.17</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 37 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      ano  c_digo_departamento departamento poblacion_5_16  \\\n",
       "0  2023.0                   68    Santander         405256   \n",
       "1  2024.0                   86     Putumayo          81100   \n",
       "2  2016.0                   52     Nariño          394,574   \n",
       "3  2024.0                   50         Meta         221911   \n",
       "4  2020.0                    5    antioquia        1151149   \n",
       "\n",
       "   tasa_matriculacion_5_16  cobertura_neta  cobertura_neta_transicion  \\\n",
       "0                    95.80           95.64                      71.27   \n",
       "1                    80.03           80.01                      50.77   \n",
       "2                    71.10             NaN                      38.56   \n",
       "3                    90.34           90.29                      65.69   \n",
       "4                    93.59           93.43                      70.25   \n",
       "\n",
       "   cobertura_neta_primaria  cobertura_neta_secundaria  cobertura_neta_media  \\\n",
       "0                    92.98                      83.40                 56.02   \n",
       "1                    78.12                      66.49                 38.12   \n",
       "2                    68.32                      56.56                 29.80   \n",
       "3                    89.17                      76.00                 48.10   \n",
       "4                    88.26                      83.38                 47.45   \n",
       "\n",
       "   ...  reprobacion  reprobacion_transicion  reprobacion_primaria  \\\n",
       "0  ...         7.65                    2.95                 91.82   \n",
       "1  ...         6.07                    3.02                 90.68   \n",
       "2  ...         1.89                    0.04                 97.71   \n",
       "3  ...         6.84                    2.85                 91.18   \n",
       "4  ...          NaN                    0.79                 92.19   \n",
       "\n",
       "   reprobacion_secundaria  reprobacion_media  repitencia  \\\n",
       "0                   12.95               6.92        7.99   \n",
       "1                   10.04               4.82        8.50   \n",
       "2                    3.07               1.92        2.64   \n",
       "3                   11.36               5.61        9.27   \n",
       "4                   11.46               7.12        6.48   \n",
       "\n",
       "   repitencia_transicion  repitencia_primaria  repitencia_secundaria  \\\n",
       "0                   2.95                 7.32                  11.34   \n",
       "1                   3.02                 7.56                  12.33   \n",
       "2                   0.04                 2.36                   3.79   \n",
       "3                   2.85                 9.27                  12.53   \n",
       "4                   0.79                 5.92                   9.37   \n",
       "\n",
       "   repitencia_media  \n",
       "0              4.07  \n",
       "1              4.57  \n",
       "2              1.34  \n",
       "3              3.49  \n",
       "4              3.17  \n",
       "\n",
       "[5 rows x 37 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Data Types\n",
    "\n",
    "Every column has a data type that tells pandas how to store and process the values. We use `df.dtypes` to see them.\n",
    "\n",
    "**Look for suspicious types:** a year stored as `float64` instead of `int64`, or a population number stored as `object` (text).\n",
    "\n",
    "**Run the cell below.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ano                            float64\n",
       "c_digo_departamento              int64\n",
       "departamento                       str\n",
       "poblacion_5_16                     str\n",
       "tasa_matriculacion_5_16        float64\n",
       "cobertura_neta                 float64\n",
       "cobertura_neta_transicion      float64\n",
       "cobertura_neta_primaria        float64\n",
       "cobertura_neta_secundaria      float64\n",
       "cobertura_neta_media           float64\n",
       "cobertura_bruta                float64\n",
       "cobertura_bruta_transicion     float64\n",
       "cobertura_bruta_primaria       float64\n",
       "cobertura_bruta_secundaria     float64\n",
       "cobertura_bruta_media          float64\n",
       "tamano_promedio_grupo          float64\n",
       "sedes_conectadas_a_internet    float64\n",
       "desercion                      float64\n",
       "desercion_transicion           float64\n",
       "desercion_primaria             float64\n",
       "desercion_secundaria           float64\n",
       "desercion_media                float64\n",
       "aprobacion                     float64\n",
       "aprobacion_transicion          float64\n",
       "aprobacion_primaria            float64\n",
       "aprobacion_secundaria          float64\n",
       "aprobacion_media               float64\n",
       "reprobacion                    float64\n",
       "reprobacion_transicion         float64\n",
       "reprobacion_primaria           float64\n",
       "reprobacion_secundaria         float64\n",
       "reprobacion_media              float64\n",
       "repitencia                     float64\n",
       "repitencia_transicion          float64\n",
       "repitencia_primaria            float64\n",
       "repitencia_secundaria          float64\n",
       "repitencia_media               float64\n",
       "dtype: object"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**What just happened:** pandas tells us the type of each column.\n",
    "\n",
    "**Key findings:**\n",
    "- `ano` is `float64` (decimal) but years should be whole numbers (`int64`). Years like 2023.0 look odd.\n",
    "- `poblacion_5_16` is `object` (text) even though it should be a number. This means some values contain characters that prevent pandas from reading them as numbers.\n",
    "- Most rate columns are `float64`, which is correct for percentages."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4 Missing Values\n",
    "\n",
    "`isnull().sum()` counts how many NaN (missing) values each column has. We sort the result so the worst offenders appear first.\n",
    "\n",
    "**Run the cell below.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns with missing values: 12 of 37\n",
      "Total missing cells: 877 of 17,834\n",
      "\n",
      "sedes_conectadas_a_internet    240\n",
      "tamano_promedio_grupo          240\n",
      "aprobacion                      55\n",
      "reprobacion                     53\n",
      "desercion_primaria              50\n",
      "poblacion_5_16                  50\n",
      "cobertura_neta_primaria         50\n",
      "desercion                       47\n",
      "desercion_secundaria            40\n",
      "cobertura_neta                  40\n",
      "departamento                    11\n",
      "desercion_transicion             1\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "missing = df.isnull().sum()\n",
    "missing_only = missing[missing > 0].sort_values(ascending=False)\n",
    "\n",
    "print(f\"Columns with missing values: {len(missing_only)} of {len(df.columns)}\")\n",
    "print(f\"Total missing cells: {missing.sum()} of {df.shape[0] * df.shape[1]:,}\")\n",
    "print()\n",
    "print(missing_only)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.5 Statistical Summary\n",
    "\n",
    "`df.describe()` gives us count, mean, std, min, max, and percentiles for every numeric column. This helps spot outliers and confirms what we learned from `isnull()`.\n",
    "\n",
    "**Run the cell below.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ano</th>\n",
       "      <th>c_digo_departamento</th>\n",
       "      <th>tasa_matriculacion_5_16</th>\n",
       "      <th>cobertura_neta</th>\n",
       "      <th>cobertura_neta_transicion</th>\n",
       "      <th>cobertura_neta_primaria</th>\n",
       "      <th>cobertura_neta_secundaria</th>\n",
       "      <th>cobertura_neta_media</th>\n",
       "      <th>cobertura_bruta</th>\n",
       "      <th>cobertura_bruta_transicion</th>\n",
       "      <th>...</th>\n",
       "      <th>reprobacion</th>\n",
       "      <th>reprobacion_transicion</th>\n",
       "      <th>reprobacion_primaria</th>\n",
       "      <th>reprobacion_secundaria</th>\n",
       "      <th>reprobacion_media</th>\n",
       "      <th>repitencia</th>\n",
       "      <th>repitencia_transicion</th>\n",
       "      <th>repitencia_primaria</th>\n",
       "      <th>repitencia_secundaria</th>\n",
       "      <th>repitencia_media</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>482.00</td>\n",
       "      <td>482.00</td>\n",
       "      <td>482.00</td>\n",
       "      <td>442.00</td>\n",
       "      <td>482.00</td>\n",
       "      <td>432.00</td>\n",
       "      <td>482.00</td>\n",
       "      <td>482.00</td>\n",
       "      <td>482.00</td>\n",
       "      <td>482.00</td>\n",
       "      <td>...</td>\n",
       "      <td>429.00</td>\n",
       "      <td>482.00</td>\n",
       "      <td>482.00</td>\n",
       "      <td>482.00</td>\n",
       "      <td>482.00</td>\n",
       "      <td>482.00</td>\n",
       "      <td>482.00</td>\n",
       "      <td>482.00</td>\n",
       "      <td>482.00</td>\n",
       "      <td>482.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>2017.47</td>\n",
       "      <td>51.96</td>\n",
       "      <td>85.64</td>\n",
       "      <td>85.62</td>\n",
       "      <td>57.72</td>\n",
       "      <td>84.35</td>\n",
       "      <td>68.22</td>\n",
       "      <td>38.83</td>\n",
       "      <td>98.29</td>\n",
       "      <td>89.94</td>\n",
       "      <td>...</td>\n",
       "      <td>6.10</td>\n",
       "      <td>1.45</td>\n",
       "      <td>91.62</td>\n",
       "      <td>8.24</td>\n",
       "      <td>5.20</td>\n",
       "      <td>4.27</td>\n",
       "      <td>1.45</td>\n",
       "      <td>4.15</td>\n",
       "      <td>5.56</td>\n",
       "      <td>2.23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>4.02</td>\n",
       "      <td>30.89</td>\n",
       "      <td>10.36</td>\n",
       "      <td>11.56</td>\n",
       "      <td>11.46</td>\n",
       "      <td>10.28</td>\n",
       "      <td>15.46</td>\n",
       "      <td>13.08</td>\n",
       "      <td>11.29</td>\n",
       "      <td>17.03</td>\n",
       "      <td>...</td>\n",
       "      <td>7.38</td>\n",
       "      <td>2.34</td>\n",
       "      <td>4.62</td>\n",
       "      <td>4.72</td>\n",
       "      <td>2.84</td>\n",
       "      <td>3.40</td>\n",
       "      <td>2.34</td>\n",
       "      <td>3.81</td>\n",
       "      <td>4.12</td>\n",
       "      <td>1.76</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>2011.00</td>\n",
       "      <td>5.00</td>\n",
       "      <td>52.33</td>\n",
       "      <td>-9.26</td>\n",
       "      <td>19.22</td>\n",
       "      <td>51.12</td>\n",
       "      <td>-5.80</td>\n",
       "      <td>6.79</td>\n",
       "      <td>59.79</td>\n",
       "      <td>32.03</td>\n",
       "      <td>...</td>\n",
       "      <td>-14.42</td>\n",
       "      <td>0.00</td>\n",
       "      <td>70.72</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.06</td>\n",
       "      <td>0.11</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>2014.00</td>\n",
       "      <td>20.00</td>\n",
       "      <td>80.01</td>\n",
       "      <td>80.46</td>\n",
       "      <td>51.41</td>\n",
       "      <td>78.32</td>\n",
       "      <td>60.99</td>\n",
       "      <td>31.18</td>\n",
       "      <td>93.32</td>\n",
       "      <td>81.09</td>\n",
       "      <td>...</td>\n",
       "      <td>3.24</td>\n",
       "      <td>0.22</td>\n",
       "      <td>90.30</td>\n",
       "      <td>4.44</td>\n",
       "      <td>3.26</td>\n",
       "      <td>1.54</td>\n",
       "      <td>0.22</td>\n",
       "      <td>1.34</td>\n",
       "      <td>2.04</td>\n",
       "      <td>0.87</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>2017.00</td>\n",
       "      <td>52.00</td>\n",
       "      <td>87.53</td>\n",
       "      <td>88.05</td>\n",
       "      <td>58.82</td>\n",
       "      <td>86.12</td>\n",
       "      <td>72.52</td>\n",
       "      <td>41.07</td>\n",
       "      <td>100.46</td>\n",
       "      <td>89.60</td>\n",
       "      <td>...</td>\n",
       "      <td>6.19</td>\n",
       "      <td>0.57</td>\n",
       "      <td>92.12</td>\n",
       "      <td>8.98</td>\n",
       "      <td>5.30</td>\n",
       "      <td>3.22</td>\n",
       "      <td>0.57</td>\n",
       "      <td>2.91</td>\n",
       "      <td>4.23</td>\n",
       "      <td>1.66</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>2021.00</td>\n",
       "      <td>81.00</td>\n",
       "      <td>93.46</td>\n",
       "      <td>93.43</td>\n",
       "      <td>66.16</td>\n",
       "      <td>91.46</td>\n",
       "      <td>78.81</td>\n",
       "      <td>48.08</td>\n",
       "      <td>105.92</td>\n",
       "      <td>97.88</td>\n",
       "      <td>...</td>\n",
       "      <td>7.66</td>\n",
       "      <td>1.54</td>\n",
       "      <td>94.51</td>\n",
       "      <td>11.48</td>\n",
       "      <td>6.67</td>\n",
       "      <td>6.30</td>\n",
       "      <td>1.54</td>\n",
       "      <td>5.98</td>\n",
       "      <td>8.66</td>\n",
       "      <td>3.46</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>2024.00</td>\n",
       "      <td>99.00</td>\n",
       "      <td>103.94</td>\n",
       "      <td>108.88</td>\n",
       "      <td>88.28</td>\n",
       "      <td>135.06</td>\n",
       "      <td>91.09</td>\n",
       "      <td>113.09</td>\n",
       "      <td>123.04</td>\n",
       "      <td>144.97</td>\n",
       "      <td>...</td>\n",
       "      <td>137.39</td>\n",
       "      <td>17.01</td>\n",
       "      <td>99.06</td>\n",
       "      <td>24.57</td>\n",
       "      <td>17.91</td>\n",
       "      <td>17.37</td>\n",
       "      <td>17.01</td>\n",
       "      <td>22.77</td>\n",
       "      <td>25.22</td>\n",
       "      <td>16.39</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows × 35 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           ano  c_digo_departamento  tasa_matriculacion_5_16  cobertura_neta  \\\n",
       "count   482.00               482.00                   482.00          442.00   \n",
       "mean   2017.47                51.96                    85.64           85.62   \n",
       "std       4.02                30.89                    10.36           11.56   \n",
       "min    2011.00                 5.00                    52.33           -9.26   \n",
       "25%    2014.00                20.00                    80.01           80.46   \n",
       "50%    2017.00                52.00                    87.53           88.05   \n",
       "75%    2021.00                81.00                    93.46           93.43   \n",
       "max    2024.00                99.00                   103.94          108.88   \n",
       "\n",
       "       cobertura_neta_transicion  cobertura_neta_primaria  \\\n",
       "count                     482.00                   432.00   \n",
       "mean                       57.72                    84.35   \n",
       "std                        11.46                    10.28   \n",
       "min                        19.22                    51.12   \n",
       "25%                        51.41                    78.32   \n",
       "50%                        58.82                    86.12   \n",
       "75%                        66.16                    91.46   \n",
       "max                        88.28                   135.06   \n",
       "\n",
       "       cobertura_neta_secundaria  cobertura_neta_media  cobertura_bruta  \\\n",
       "count                     482.00                482.00           482.00   \n",
       "mean                       68.22                 38.83            98.29   \n",
       "std                        15.46                 13.08            11.29   \n",
       "min                        -5.80                  6.79            59.79   \n",
       "25%                        60.99                 31.18            93.32   \n",
       "50%                        72.52                 41.07           100.46   \n",
       "75%                        78.81                 48.08           105.92   \n",
       "max                        91.09                113.09           123.04   \n",
       "\n",
       "       cobertura_bruta_transicion  ...  reprobacion  reprobacion_transicion  \\\n",
       "count                      482.00  ...       429.00                  482.00   \n",
       "mean                        89.94  ...         6.10                    1.45   \n",
       "std                         17.03  ...         7.38                    2.34   \n",
       "min                         32.03  ...       -14.42                    0.00   \n",
       "25%                         81.09  ...         3.24                    0.22   \n",
       "50%                         89.60  ...         6.19                    0.57   \n",
       "75%                         97.88  ...         7.66                    1.54   \n",
       "max                        144.97  ...       137.39                   17.01   \n",
       "\n",
       "       reprobacion_primaria  reprobacion_secundaria  reprobacion_media  \\\n",
       "count                482.00                  482.00             482.00   \n",
       "mean                  91.62                    8.24               5.20   \n",
       "std                    4.62                    4.72               2.84   \n",
       "min                   70.72                    0.00               0.00   \n",
       "25%                   90.30                    4.44               3.26   \n",
       "50%                   92.12                    8.98               5.30   \n",
       "75%                   94.51                   11.48               6.67   \n",
       "max                   99.06                   24.57              17.91   \n",
       "\n",
       "       repitencia  repitencia_transicion  repitencia_primaria  \\\n",
       "count      482.00                 482.00               482.00   \n",
       "mean         4.27                   1.45                 4.15   \n",
       "std          3.40                   2.34                 3.81   \n",
       "min          0.10                   0.00                 0.06   \n",
       "25%          1.54                   0.22                 1.34   \n",
       "50%          3.22                   0.57                 2.91   \n",
       "75%          6.30                   1.54                 5.98   \n",
       "max         17.37                  17.01                22.77   \n",
       "\n",
       "       repitencia_secundaria  repitencia_media  \n",
       "count                 482.00            482.00  \n",
       "mean                    5.56              2.23  \n",
       "std                     4.12              1.76  \n",
       "min                     0.11              0.00  \n",
       "25%                     2.04              0.87  \n",
       "50%                     4.23              1.66  \n",
       "75%                     8.66              3.46  \n",
       "max                    25.22             16.39  \n",
       "\n",
       "[8 rows x 35 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.describe().round(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**What just happened:** The `count` row shows how many non-null values each column has. Columns where `count` is less than 482 (our total rows) have missing data. Also look at `min` and `max`: do any values look impossible? We will come back to this in Section 6.\n",
    "\n",
    "### Inspection Summary\n",
    "\n",
    "From our inspection, we found **5 data quality issues** to fix:\n",
    "\n",
    "| # | Issue | Where |\n",
    "|---|-------|-------|\n",
    "| 1 | Missing values (NaN) | Multiple columns, especially `sedes_conectadas_a_internet`, `tamano_promedio_grupo` |\n",
    "| 2 | Wrong data types | `ano` is float64, `poblacion_5_16` is object |\n",
    "| 3 | Duplicate rows | 482 rows but only 462 expected (20 extra) |\n",
    "| 4 | Text inconsistencies | `departamento` has too many unique values |\n",
    "| 5 | Invalid values | Some percentages might be negative or above 100 |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### QUESTION 1\n",
    "\n",
    "Based on the inspection you just ran, **list 3 specific problems** you noticed. For each one, note:\n",
    "- Which column is affected\n",
    "- What the problem is\n",
    "- How you spotted it (which command revealed it)\n",
    "\n",
    "*Double-click this cell and write your answer below:*\n",
    "\n",
    "1.   valores faltantes (NaN) en varias columnas como `sedes_conectadas_a_internet`, `tamano_promedio_grupo`\n",
    "2. Inconsistencia en Formatos y Mayúsculas (Ruido en Texto)\n",
    "3. valores erroneos en las columnas que no tienen sentido"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Section 2: Missing Values\n",
    "\n",
    "**What is NaN?** NaN stands for \"Not a Number.\" It is pandas' way of saying \"this value is missing.\" Think of it like reaching into your sock drawer and finding... nothing. The sock is not there. It is not zero socks (that would mean you counted and found none). It is \"unknown.\"\n",
    "\n",
    "**Why do missing values appear?** Data can be missing because:\n",
    "- It was never collected (a survey question left blank)\n",
    "- It was lost during processing (a system error)\n",
    "- It does not apply (internet connectivity data before internet existed in schools)\n",
    "\n",
    "Like socks without a pair: you need to decide whether to find a replacement, toss them, or accept the mismatch."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Missing Value Percentages\n",
    "\n",
    "Before deciding what to do, we need to know how bad the problem is. Let's calculate the percentage of missing values per column.\n",
    "\n",
    "**Run the cell below.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing value percentages:\n",
      "sedes_conectadas_a_internet    49.79\n",
      "tamano_promedio_grupo          49.79\n",
      "aprobacion                     11.41\n",
      "reprobacion                    11.00\n",
      "desercion_primaria             10.37\n",
      "poblacion_5_16                 10.37\n",
      "cobertura_neta_primaria        10.37\n",
      "desercion                       9.75\n",
      "desercion_secundaria            8.30\n",
      "cobertura_neta                  8.30\n",
      "departamento                    2.28\n",
      "desercion_transicion            0.21\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "missing_pct = (df.isnull().sum() / len(df) * 100).round(2)\n",
    "missing_pct = missing_pct[missing_pct > 0].sort_values(ascending=False)\n",
    "\n",
    "print(\"Missing value percentages:\")\n",
    "print(missing_pct)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 The Decision Framework\n",
    "\n",
    "Now we apply the decision framework from class:\n",
    "\n",
    "| Missing % | Action | Reasoning |\n",
    "|-----------|--------|----------|\n",
    "| > 50% | Consider dropping the column | More gaps than data |\n",
    "| < 5% | Safe to drop the rows | Losing very few rows |\n",
    "| 5-50% | Fill with an appropriate value | Too many rows to lose, need to estimate |\n",
    "\n",
    "Applying this to our columns:\n",
    "- `sedes_conectadas_a_internet`, `tamano_promedio_grupo`: ~50% missing. Fill with 0 (\"not reported\").\n",
    "- `departamento`: ~2-3% missing. Drop those rows (critical identifier, cannot guess).\n",
    "- Rate columns (`desercion`, `cobertura_neta`, `aprobacion`, `reprobacion`): 8-11% missing. Fill with median.\n",
    "- `poblacion_5_16`: ~10% missing. We will fix this in Section 3 (it also has type problems)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Fill Count-Based Columns with 0\n",
    "\n",
    "For `sedes_conectadas_a_internet` (% schools with internet) and `tamano_promedio_grupo` (average class size), the data only exists through 2017. After that, it was not reported. Filling with 0 means \"no data available for this period.\"\n",
    "\n",
    "**Run the cell below.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before fillna:\n",
      "  sedes_conectadas_a_internet NaN: 240\n",
      "  tamano_promedio_grupo NaN:       240\n",
      "\n",
      "After fillna:\n",
      "  sedes_conectadas_a_internet NaN: 0\n",
      "  tamano_promedio_grupo NaN:       0\n"
     ]
    }
   ],
   "source": [
    "print(f\"Before fillna:\")\n",
    "print(f\"  sedes_conectadas_a_internet NaN: {df['sedes_conectadas_a_internet'].isnull().sum()}\")\n",
    "print(f\"  tamano_promedio_grupo NaN:       {df['tamano_promedio_grupo'].isnull().sum()}\")\n",
    "\n",
    "df['sedes_conectadas_a_internet'] = df['sedes_conectadas_a_internet'].fillna(0)\n",
    "df['tamano_promedio_grupo'] = df['tamano_promedio_grupo'].fillna(0)\n",
    "\n",
    "print(f\"\\nAfter fillna:\")\n",
    "print(f\"  sedes_conectadas_a_internet NaN: {df['sedes_conectadas_a_internet'].isnull().sum()}\")\n",
    "print(f\"  tamano_promedio_grupo NaN:       {df['tamano_promedio_grupo'].isnull().sum()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**What just happened:** We replaced all NaN values in those two columns with 0. The missing count went from ~240 to 0 for each.\n",
    "\n",
    "**Why 0 is appropriate here but NOT for rates:** These columns represent counts or measurements that simply were not reported after 2017. Using 0 signals \"not available.\" But filling a dropout rate with 0 would be misleading: 0% dropout means \"nobody dropped out\" (a strong claim), not \"we don't know.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 Drop Rows Where `departamento` is Missing\n",
    "\n",
    "The `departamento` column is a critical identifier. A row without a department is like a letter without an address: useless. We cannot guess which department it belongs to, so we remove those rows.\n",
    "\n",
    "**Run the cell below.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rows before: 482\n",
      "Rows after:  471\n",
      "Removed:     11 rows (missing departamento)\n"
     ]
    }
   ],
   "source": [
    "rows_before = len(df)\n",
    "\n",
    "df = df.dropna(subset=['departamento'])\n",
    "\n",
    "rows_after = len(df)\n",
    "print(f\"Rows before: {rows_before}\")\n",
    "print(f\"Rows after:  {rows_after}\")\n",
    "print(f\"Removed:     {rows_before - rows_after} rows (missing departamento)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.5 Fill Rate Columns with the Median\n",
    "\n",
    "For columns that represent rates or percentages (dropout rate, coverage, approval, etc.), we fill with the **median**. The median is the middle value when all values are sorted. It is better than the mean because it is not distorted by extreme outliers.\n",
    "\n",
    "**Why not fill with 0?** A 0% dropout rate means \"nobody dropped out.\" That is very different from \"we don't know.\" The median says: \"if we had to guess, the most typical value is probably close to this.\"\n",
    "\n",
    "**Run the cell below.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cobertura_neta: filled 37 NaN with median 87.95\n",
      "cobertura_neta_primaria: filled 49 NaN with median 86.07\n",
      "desercion: filled 46 NaN with median 3.82\n",
      "desercion_transicion: filled 1 NaN with median 3.78\n",
      "desercion_primaria: filled 47 NaN with median 3.15\n",
      "desercion_secundaria: filled 39 NaN with median 4.75\n",
      "aprobacion: filled 53 NaN with median 89.97\n",
      "reprobacion: filled 53 NaN with median 6.25\n",
      "\n",
      "Total NaN remaining in rate columns: 0\n"
     ]
    }
   ],
   "source": [
    "rate_columns = [\n",
    "    'tasa_matriculacion_5_16',\n",
    "    'cobertura_neta', 'cobertura_neta_transicion', 'cobertura_neta_primaria',\n",
    "    'cobertura_neta_secundaria', 'cobertura_neta_media',\n",
    "    'cobertura_bruta', 'cobertura_bruta_transicion', 'cobertura_bruta_primaria',\n",
    "    'cobertura_bruta_secundaria', 'cobertura_bruta_media',\n",
    "    'desercion', 'desercion_transicion', 'desercion_primaria',\n",
    "    'desercion_secundaria', 'desercion_media',\n",
    "    'aprobacion', 'aprobacion_transicion', 'aprobacion_primaria',\n",
    "    'aprobacion_secundaria', 'aprobacion_media',\n",
    "    'reprobacion', 'reprobacion_transicion', 'reprobacion_primaria',\n",
    "    'reprobacion_secundaria', 'reprobacion_media',\n",
    "    'repitencia', 'repitencia_transicion', 'repitencia_primaria',\n",
    "    'repitencia_secundaria', 'repitencia_media',\n",
    "]\n",
    "\n",
    "for col in rate_columns:\n",
    "    n_missing = df[col].isnull().sum()\n",
    "    if n_missing > 0:\n",
    "        median_val = df[col].median()\n",
    "        df[col] = df[col].fillna(median_val)\n",
    "        print(f\"{col}: filled {n_missing} NaN with median {median_val:.2f}\")\n",
    "\n",
    "print(f\"\\nTotal NaN remaining in rate columns: {df[rate_columns].isnull().sum().sum()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**What just happened:** Each rate column's missing values were replaced with that column's median. For example, if the median dropout rate is 3.86%, all missing dropout values now say 3.86%. This is a reasonable assumption: \"if we don't know the value, assume it is typical.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.6 Verify: How Many Missing Values Remain?\n",
    "\n",
    "Let's check how our cleaning is going so far.\n",
    "\n",
    "**Run the cell below.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns still with missing values: 1\n",
      "poblacion_5_16    46\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "remaining = df.isnull().sum()\n",
    "remaining = remaining[remaining > 0]\n",
    "\n",
    "if len(remaining) == 0:\n",
    "    print(\"No missing values remain!\")\n",
    "else:\n",
    "    print(f\"Columns still with missing values: {len(remaining)}\")\n",
    "    print(remaining)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**What to expect:** The only column with missing values should be `poblacion_5_16`. We will fix it in Section 3, because its problem is not just missing values but also wrong data types (it has commas and the text \"sin dato\").\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### QUESTION 2\n",
    "\n",
    "Why would filling `desercion` (dropout rate) with 0 be misleading? What does 0% dropout actually mean vs. \"no data\"?\n",
    "\n",
    "*Double-click this cell and write your answer below:*\n",
    "\n",
    "Llenar los espacios vacíos con 0 en la columna de desercion es un error común pero peligroso en el análisis de datos. 0% es un resultado perfecto, indicador de exito total en la retencion escolar y NaN significa ignorancia al hecho de saber que la informacion se perdio o no se reporto"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Section 3: Data Type Issues\n",
    "\n",
    "Imagine sorting your laundry and finding **a shoe in the shirt pile**. It does not belong there, and it will cause problems in the wash. That is what happens when a number is stored as text: pandas cannot do math with it.\n",
    "\n",
    "There are three main types we care about:\n",
    "- `int64`: whole numbers (years, counts)\n",
    "- `float64`: decimal numbers (rates, percentages)\n",
    "- `object`: text/strings (names, categories)\n",
    "\n",
    "We found two type problems:\n",
    "1. `ano` (year): stored as `float64` (2023.0) instead of `int64` (2023)\n",
    "2. `poblacion_5_16` (population): stored as `object` (text) instead of `int64`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Inspect `ano`\n",
    "\n",
    "Let's see what the `ano` column looks like right now.\n",
    "\n",
    "**Run the cell below.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dtype: float64\n",
      "\n",
      "Sample values: [2023.0, 2024.0, 2016.0, 2024.0, 2020.0, 2015.0, 2022.0, 2023.0, 2011.0, 2017.0]\n"
     ]
    }
   ],
   "source": [
    "print(f\"dtype: {df['ano'].dtype}\")\n",
    "print(f\"\\nSample values: {df['ano'].head(10).tolist()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Fix `ano`: Convert to Integer\n",
    "\n",
    "Years should be whole numbers. The CSV had mixed formats like \"2011\" and \"2011.0\", which caused pandas to read the column as float. We fix it by first ensuring all values are numeric with `pd.to_numeric()`, then converting to int.\n",
    "\n",
    "**Run the cell below.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before: ano dtype = float64\n",
      "Sample: [2023.0, 2024.0, 2016.0, 2024.0, 2020.0]\n",
      "\n",
      "After: ano dtype = int64\n",
      "Sample: [2023, 2024, 2016, 2024, 2020]\n"
     ]
    }
   ],
   "source": [
    "print(f\"Before: ano dtype = {df['ano'].dtype}\")\n",
    "print(f\"Sample: {df['ano'].head(5).tolist()}\")\n",
    "\n",
    "df['ano'] = pd.to_numeric(df['ano'], errors='coerce').fillna(0).astype(int)\n",
    "\n",
    "print(f\"\\nAfter: ano dtype = {df['ano'].dtype}\")\n",
    "print(f\"Sample: {df['ano'].head(5).tolist()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**What just happened:** The years changed from 2023.0 (float) to 2023 (integer). Clean and correct.\n",
    "\n",
    "**The NaN-before-int trap:** If `ano` had NaN values and we tried `astype(int)` directly, pandas would crash. The `fillna(0)` step prevents that. Always fill NaN before converting to int."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Inspect `poblacion_5_16`\n",
    "\n",
    "This column is text (`object`) instead of numeric. Let's look at the raw values to understand why.\n",
    "\n",
    "**Run the cell below.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current dtype: str\n",
      "\n",
      "Sample values (10 random):\n",
      "['15395', '327,302', '533666', '1288473', '22896', '161892', '544473', '487274', '393,195', '573948']\n"
     ]
    }
   ],
   "source": [
    "print(f\"Current dtype: {df['poblacion_5_16'].dtype}\")\n",
    "print(f\"\\nSample values (10 random):\")\n",
    "print(df['poblacion_5_16'].dropna().sample(10, random_state=42).tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**What to notice:** Some values have commas (like \"394,574\") and some say \"sin dato\" (Spanish for \"no data\"). Pandas cannot convert these to numbers automatically, so it stored the whole column as text.\n",
    "\n",
    "### 3.4 Fix `poblacion_5_16`: Clean String, Then Convert\n",
    "\n",
    "The fix is a 2-step process:\n",
    "1. **Remove the commas** with `str.replace(',', '')`\n",
    "2. **Convert to numeric** with `pd.to_numeric(errors='coerce')` so \"sin dato\" becomes NaN instead of crashing\n",
    "\n",
    "Then we fill the remaining NaN with 0 and convert to int.\n",
    "\n",
    "**Run the cell below.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After to_numeric: dtype = float64\n",
      "NaN count: 78\n",
      "\n",
      "Final dtype: int64\n",
      "NaN remaining: 0\n",
      "Sample: [405256, 81100, 394574, 221911, 1151149]\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Remove commas, then convert to numeric\n",
    "df['poblacion_5_16'] = pd.to_numeric(\n",
    "    df['poblacion_5_16'].astype(str).str.replace(',', ''),\n",
    "    errors='coerce'\n",
    ")\n",
    "\n",
    "print(f\"After to_numeric: dtype = {df['poblacion_5_16'].dtype}\")\n",
    "print(f\"NaN count: {df['poblacion_5_16'].isnull().sum()}\")\n",
    "\n",
    "# Step 2: Fill NaN with 0 and convert to int\n",
    "df['poblacion_5_16'] = df['poblacion_5_16'].fillna(0).astype(int)\n",
    "\n",
    "print(f\"\\nFinal dtype: {df['poblacion_5_16'].dtype}\")\n",
    "print(f\"NaN remaining: {df['poblacion_5_16'].isnull().sum()}\")\n",
    "print(f\"Sample: {df['poblacion_5_16'].head(5).tolist()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**What just happened:**\n",
    "- Commas were removed: \"394,574\" became \"394574\"\n",
    "- `pd.to_numeric()` converted valid numbers to float64\n",
    "- \"sin dato\" and NaN values became NaN (the `errors='coerce'` flag replaces anything it cannot convert with NaN, instead of crashing)\n",
    "- Finally, we filled NaN with 0 and converted to int"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### YOUR TURN 1\n",
    "\n",
    "Verify the type fixes worked. Write code to print the `dtype` and 5 sample values for both `ano` and `poblacion_5_16`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columna: ano\n",
      "dtype: float64\n",
      "Muestra: [2023.0, 2024.0, 2016.0, 2024.0, 2020.0]\n",
      "\n",
      "Columna: poblacion_5_16\n",
      "dtype: str\n",
      "Muestra: ['405256', '81100', '394,574', '221911', '1151149']\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"data/educacion_estadisticas.csv\")  # ← pon aquí el nombre real del archivo\n",
    "print(\"Columna: ano\")\n",
    "print(\"dtype:\", df['ano'].dtype)\n",
    "print(\"Muestra:\", df['ano'].head(5).tolist())\n",
    "\n",
    "print(\"\\nColumna: poblacion_5_16\")\n",
    "print(\"dtype:\", df['poblacion_5_16'].dtype)\n",
    "print(\"Muestra:\", df['poblacion_5_16'].head(5).tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Section 4: Duplicates\n",
    "\n",
    "Imagine you are folding laundry and you count the **same shirt twice**. Your count is now wrong. That is exactly what duplicate rows do: they inflate counts and distort averages. If a department appears twice for the same year with identical data, every calculation using that data is biased.\n",
    "\n",
    "### 4.1 Count Duplicates\n",
    "\n",
    "We use `duplicated().sum()` to count how many duplicate rows exist.\n",
    "\n",
    "**Run the cell below.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Duplicate rows found: 9\n"
     ]
    }
   ],
   "source": [
    "n_dupes = df.duplicated().sum()\n",
    "print(f\"Duplicate rows found: {n_dupes}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 See the Duplicates\n",
    "\n",
    "Let's look at the actual duplicate rows. Using `keep=False` marks ALL copies (both the \"original\" and the \"duplicate\") so we can see them side by side.\n",
    "\n",
    "**Run the cell below.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total rows involved in duplicates: 18\n",
      "\n",
      "        ano  departamento poblacion_5_16  desercion\n",
      "68   2017.0  Bogotá, D.C.        1452849       1.62\n",
      "357  2017.0  Bogotá, D.C.        1452849       1.62\n",
      "21   2014.0       Bolívar         490573       3.53\n",
      "481  2014.0       Bolívar         490573       3.53\n",
      "27   2020.0       Bolívar         466941       2.03\n",
      "375  2020.0       Bolívar         466941       2.03\n",
      "109  2022.0      Casanare          95609       3.98\n",
      "430  2022.0      Casanare          95609       3.98\n",
      "240  2015.0         Chocó         145625       4.78\n",
      "255  2015.0         Chocó         145625       4.78\n",
      "8    2011.0     Magdalena         333793       3.63\n",
      "376  2011.0     Magdalena         333793       3.63\n",
      "12   2014.0     Magdalena            NaN       2.96\n",
      "117  2014.0     Magdalena            NaN       2.96\n",
      "286  2016.0     Magdalena         329234       3.20\n",
      "291  2016.0     Magdalena         329234       3.20\n",
      "1    2024.0      Putumayo          81100       6.49\n",
      "418  2024.0      Putumayo          81100       6.49\n"
     ]
    }
   ],
   "source": [
    "dupes = df[df.duplicated(keep=False)].sort_values(['departamento', 'ano'])\n",
    "print(f\"Total rows involved in duplicates: {len(dupes)}\")\n",
    "print()\n",
    "print(dupes[['ano', 'departamento', 'poblacion_5_16', 'desercion']].head(20).to_string())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Remove Duplicates\n",
    "\n",
    "`drop_duplicates()` keeps the first occurrence of each row and removes the rest.\n",
    "\n",
    "**Run the cell below.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rows before: 482\n",
      "Rows after:  473\n",
      "Removed:     9 duplicate rows\n",
      "Duplicates remaining: 0\n"
     ]
    }
   ],
   "source": [
    "rows_before = len(df)\n",
    "\n",
    "df = df.drop_duplicates()\n",
    "\n",
    "rows_after = len(df)\n",
    "print(f\"Rows before: {rows_before}\")\n",
    "print(f\"Rows after:  {rows_after}\")\n",
    "print(f\"Removed:     {rows_before - rows_after} duplicate rows\")\n",
    "print(f\"Duplicates remaining: {df.duplicated().sum()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**What just happened:** The duplicate rows were removed. We kept one copy of each and deleted the rest. The 20 duplicates were exact copies injected into the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### QUESTION 3\n",
    "\n",
    "When might a duplicate row be **valid** and NOT an error? Give one example from the real world.\n",
    "\n",
    "*Double-click this cell and write your answer below:*\n",
    "Un duplicado puede ser válido cuando representa eventos distintos que comparten exactamente los mismos valores en todas las columnas.\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Section 5: Text Inconsistencies\n",
    "\n",
    "Imagine you are folding laundry but your labels are **inconsistent**: one shirt says \"Blue\", another says \"BLUE\", another says \"  blue  \". To you, they are the same color. But to pandas, these are three completely different values. This breaks any grouping or counting operation.\n",
    "\n",
    "### 5.1 Explore the Problem\n",
    "\n",
    "Colombia has about 34 departments (32 + Bogota D.C. + national aggregate). Let's see how many unique values our `departamento` column actually has.\n",
    "\n",
    "**Run the cell below.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique department values: 122\n",
      "(We expect about 34)\n",
      "\n",
      "All unique values (sorted):\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'<' not supported between instances of 'float' and 'str'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[10]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      2\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m(We expect about 34)\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m      3\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mAll unique values (sorted):\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m val \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28;43msorted\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mdf\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mdepartamento\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m.\u001b[49m\u001b[43munique\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[32m      5\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m  \u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mval\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mTypeError\u001b[39m: '<' not supported between instances of 'float' and 'str'"
     ]
    }
   ],
   "source": [
    "print(f\"Unique department values: {df['departamento'].nunique()}\")\n",
    "print(f\"(We expect about 34)\")\n",
    "print(f\"\\nAll unique values (sorted):\")\n",
    "for val in sorted(df['departamento'].unique()):\n",
    "    print(f\"  '{val}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**What just happened:** We see far more unique values than expected. The same department appears in multiple forms:\n",
    "- \"Antioquia\", \"ANTIOQUIA\", \"  Antioquia  \", \"antioquia\" are all the same department\n",
    "- Some have leading/trailing spaces\n",
    "- Some have accents stripped (\"Narino\" vs \"Nariño\")\n",
    "\n",
    "To pandas, every single variation is a completely different string."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 Standardize Text\n",
    "\n",
    "The fix: convert everything to uppercase and remove extra whitespace with `str.upper().str.strip()`.\n",
    "\n",
    "**Run the cell below.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique values before: 122\n",
      "Unique values after:  41\n",
      "Reduced by:           81 values\n"
     ]
    }
   ],
   "source": [
    "before_nunique = df['departamento'].nunique()\n",
    "\n",
    "df['departamento'] = df['departamento'].str.upper().str.strip()\n",
    "\n",
    "after_nunique = df['departamento'].nunique()\n",
    "\n",
    "print(f\"Unique values before: {before_nunique}\")\n",
    "print(f\"Unique values after:  {after_nunique}\")\n",
    "print(f\"Reduced by:           {before_nunique - after_nunique} values\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**What just happened:** All department names were converted to uppercase and extra spaces were removed. The number of unique values dropped significantly. Now \"Antioquia\", \"ANTIOQUIA\", \"  antioquia  \" are all just \"ANTIOQUIA\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3 Check for New Duplicates\n",
    "\n",
    "Here is a subtle but important lesson: **cleaning one thing can reveal new problems.** Rows that looked different before (\"Antioquia\" vs \"ANTIOQUIA\" for the same year) are now identical after uppercasing. We need to check for duplicates again.\n",
    "\n",
    "**Run the cell below.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New duplicates after text standardization: 7\n",
      "Removed 7 additional duplicates\n",
      "Final row count: 466\n"
     ]
    }
   ],
   "source": [
    "new_dupes = df.duplicated().sum()\n",
    "print(f\"New duplicates after text standardization: {new_dupes}\")\n",
    "\n",
    "if new_dupes > 0:\n",
    "    rows_before = len(df)\n",
    "    df = df.drop_duplicates()\n",
    "    rows_after = len(df)\n",
    "    print(f\"Removed {rows_before - rows_after} additional duplicates\")\n",
    "    print(f\"Final row count: {rows_after}\")\n",
    "else:\n",
    "    print(\"No new duplicates created. Good.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Lesson:** Data cleaning is iterative. Fixing one problem (text inconsistency) can create another (new duplicates). Always verify after each step."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### YOUR TURN 2\n",
    "\n",
    "After standardization, verify how many unique departments we have now. Print the sorted list of unique department names. Does the count look reasonable for Colombia?\n",
    "\n",
    "Write your code below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Número de departamentos únicos: 34\n",
      "\n",
      "Lista de departamentos (ordenada):\n",
      "AMAZONAS\n",
      "ANTIOQUIA\n",
      "ARAUCA\n",
      "ARCHIPIÉLAGO DE SAN ANDRÉS, PROVIDENCIA Y SANTA CATALINA\n",
      "ATLANTICO\n",
      "BOGOTA, D.C.\n",
      "BOLIVAR\n",
      "BOYACA\n",
      "CALDAS\n",
      "CAQUETA\n",
      "CASANARE\n",
      "CAUCA\n",
      "CESAR\n",
      "CHOCO\n",
      "CORDOBA\n",
      "CUNDINAMARCA\n",
      "GUAINIA\n",
      "GUAVIARE\n",
      "HUILA\n",
      "LA GUAJIRA\n",
      "MAGDALENA\n",
      "META\n",
      "NARINO\n",
      "NORTE DE SANTANDER\n",
      "PUTUMAYO\n",
      "QUINDIO\n",
      "RISARALDA\n",
      "SANTANDER\n",
      "SIN_DATO\n",
      "SUCRE\n",
      "TOLIMA\n",
      "VALLE DEL CAUCA\n",
      "VAUPES\n",
      "VICHADA\n"
     ]
    }
   ],
   "source": [
    "# Your code here: print nunique() and sorted list of unique departments\n",
    "import pandas as pd\n",
    "import unicodedata\n",
    "\n",
    "#  Función para quitar acentos\n",
    "def quitar_acentos(texto):\n",
    "    if pd.isnull(texto):\n",
    "        return \"\"\n",
    "    return ''.join(c for c in unicodedata.normalize('NFD', str(texto))\n",
    "                   if unicodedata.category(c) != 'Mn')\n",
    "\n",
    "#  Convertir a string, quitar acentos, mayúsculas y espacios extra\n",
    "df['departamento'] = df['departamento'].astype(str)          # todo a string\n",
    "df['departamento'] = df['departamento'].apply(quitar_acentos)  # quitar tildes\n",
    "df['departamento'] = df['departamento'].str.upper().str.strip() # mayúsculas y espacios\n",
    "\n",
    "#  Diccionario para corregir nombres mal escritos\n",
    "correcciones = {\n",
    "    \"BOGOTA, D,C,\": \"BOGOTA, D.C.\",\n",
    "    \"BOGOTA, D.C\": \"BOGOTA, D.C.\",\n",
    "    \"ARCHIPIELAGO DE SAN ANDRES, PROVIDENCIA Y SANTA CATALINA\": \n",
    "        \"ARCHIPIÉLAGO DE SAN ANDRÉS, PROVIDENCIA Y SANTA CATALINA\"\n",
    "}\n",
    "df['departamento'] = df['departamento'].replace(correcciones)\n",
    "\n",
    "#  Eliminar filas duplicadas generadas por la limpieza\n",
    "df = df.drop_duplicates()\n",
    "\n",
    "# Verificación final: nunique y lista ordenada\n",
    "print(\"Número de departamentos únicos:\", df['departamento'].nunique())\n",
    "print(\"\\nLista de departamentos (ordenada):\")\n",
    "for dep in sorted(df['departamento'].unique()):\n",
    "    print(dep)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Section 6: Invalid Values\n",
    "\n",
    "So far we have fixed missing values, wrong types, duplicates, and text inconsistencies. But there is one more problem hiding in the data: **values that exist but are impossible.**\n",
    "\n",
    "Think of it this way: you finished your laundry, everything is folded and sorted. But then you notice a shirt labeled \"Size -3\" and another labeled \"Size 250.\" Those sizes do not exist. The labels are wrong.\n",
    "\n",
    "In our dataset, percentage columns (dropout rate, coverage, approval, etc.) must be between 0 and 100. A dropout rate of -5% is impossible. A coverage of 150% is impossible (for net coverage). These values passed all our previous checks because they are not missing, they are not the wrong type, and they are not duplicates. **They are just wrong.**\n",
    "\n",
    "Catching these requires **domain knowledge**: knowing what valid values look like for your specific data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.1 Check Min and Max of Percentage Columns\n",
    "\n",
    "Let's use `describe()` on just the percentage columns to check their minimum and maximum values. Any `min` below 0 or `max` above 100 is suspicious.\n",
    "\n",
    "**Run the cell below.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Min and Max for percentage columns:\n",
      "     desercion  desercion_primaria  desercion_secundaria  desercion_media  cobertura_neta  cobertura_neta_primaria  cobertura_neta_secundaria  cobertura_neta_media  aprobacion  reprobacion\n",
      "min       0.46                0.34                -13.75           -10.24           -9.26                    51.12                      -5.80                  6.79       72.00       -14.42\n",
      "max      10.94              118.53                 13.18           139.00          108.88                   135.06                      91.09                113.09       99.08       137.39\n"
     ]
    }
   ],
   "source": [
    "percentage_cols = [\n",
    "    'desercion', 'desercion_primaria', 'desercion_secundaria', 'desercion_media',\n",
    "    'cobertura_neta', 'cobertura_neta_primaria', 'cobertura_neta_secundaria', 'cobertura_neta_media',\n",
    "    'aprobacion', 'reprobacion',\n",
    "]\n",
    "\n",
    "summary = df[percentage_cols].describe().loc[['min', 'max']].round(2)\n",
    "print(\"Min and Max for percentage columns:\")\n",
    "print(summary.to_string())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**What just happened:** Look at the `min` row. Do you see any negative values? Now look at the `max` row. Do you see anything above 100? Those are invalid values that should not exist in percentage columns."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2 Count the Invalid Values\n",
    "\n",
    "Let's count exactly how many values are below 0 and above 100 in each column.\n",
    "\n",
    "**Run the cell below.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Negative values (< 0) per column:\n",
      "desercion_secundaria         2\n",
      "desercion_media              2\n",
      "cobertura_neta               1\n",
      "cobertura_neta_secundaria    1\n",
      "reprobacion                  2\n",
      "\n",
      "Total negative values: 8\n",
      "\n",
      "========================================\n",
      "\n",
      "Values over 100 per column:\n",
      "desercion_primaria          1\n",
      "desercion_media             1\n",
      "cobertura_neta             11\n",
      "cobertura_neta_primaria     9\n",
      "cobertura_neta_media        1\n",
      "reprobacion                 1\n",
      "\n",
      "Total values over 100: 24\n"
     ]
    }
   ],
   "source": [
    "negatives = df[percentage_cols].lt(0).sum()\n",
    "over_100 = df[percentage_cols].gt(100).sum()\n",
    "\n",
    "print(\"Negative values (< 0) per column:\")\n",
    "print(negatives[negatives > 0].to_string())\n",
    "print(f\"\\nTotal negative values: {negatives.sum()}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*40)\n",
    "\n",
    "print(\"\\nValues over 100 per column:\")\n",
    "print(over_100[over_100 > 0].to_string())\n",
    "print(f\"\\nTotal values over 100: {over_100.sum()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.3 See the Invalid Rows\n",
    "\n",
    "Let's look at the actual rows with invalid values so we can understand the scope of the problem.\n",
    "\n",
    "**Run the cell below.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rows with negative values: 8\n",
      "        ano departamento  desercion  desercion_primaria  desercion_secundaria  desercion_media  cobertura_neta  cobertura_neta_primaria  cobertura_neta_secundaria  cobertura_neta_media  aprobacion  reprobacion\n",
      "33   2013.0        HUILA       4.78                3.27                  7.00             4.99           -9.26                    90.86                      75.27                 39.07       92.18         3.04\n",
      "44   2011.0      QUINDIO       5.04                4.52                  5.98             4.10           90.83                    85.53                      -5.80                 47.29       92.43         2.53\n",
      "139  2011.0       CALDAS       4.13                3.92                -13.75             2.82           86.98                    83.86                      73.41                 45.38       90.62         5.25\n",
      "153  2016.0        CAUCA       2.84                 NaN                  4.25           -10.00           83.84                    85.12                      65.49                 33.56       91.42         5.73\n",
      "180  2020.0        CAUCA       1.59                1.06                  2.08             2.40           84.97                    85.48                      70.04                 38.72       91.93        -9.10\n",
      "200  2021.0        HUILA       6.18                4.92                 -1.69             6.17             NaN                    93.29                      82.21                 48.19       83.98         9.84\n",
      "391  2018.0    SANTANDER       3.20                2.41                  4.10           -10.24           96.06                    94.88                      83.83                 49.63       89.46         7.34\n",
      "451  2015.0     AMAZONAS       4.76                2.53                  8.00             6.69           81.46                    82.75                      59.41                 26.52       89.23       -14.42\n",
      "\n",
      "Rows with values > 100: 20\n",
      "        ano  departamento  desercion  desercion_primaria  desercion_secundaria  desercion_media  cobertura_neta  cobertura_neta_primaria  cobertura_neta_secundaria  cobertura_neta_media  aprobacion  reprobacion\n",
      "29   2013.0        BOYACA       2.45              118.53                  3.25             2.50           85.05                    81.56                      75.09                 48.43       93.41         4.14\n",
      "39   2023.0  CUNDINAMARCA       3.77                 NaN                  4.83           139.00           93.32                    88.71                      85.25                 56.55         NaN         6.61\n",
      "47   2014.0      CASANARE        NaN                2.08                  2.64             2.20          103.24                   102.10                      84.71                 46.18       92.87         4.86\n",
      "88   2011.0     RISARALDA       4.79                3.77                  6.14             5.39           95.43                   135.06                      76.38                 42.83         NaN         1.14\n",
      "98   2011.0      CASANARE       3.88                3.40                  4.60             4.18          101.05                    99.47                      82.56                 42.64       92.12          NaN\n",
      "104  2023.0        NARINO        NaN                1.56                  3.01             1.98          108.88                    73.86                      64.71                 42.43         NaN         5.13\n",
      "160  2015.0      CASANARE       3.75                 NaN                  5.04             2.59          102.39                   102.00                      83.12                 46.95       88.89         7.36\n",
      "177  2016.0       BOLIVAR       2.97                2.45                  3.71             2.48           89.41                    88.01                      70.99                 42.55       91.68       137.39\n",
      "309  2023.0    LA GUAJIRA       3.29                 NaN                  3.92             2.45           98.91                   100.85                      60.98                 29.96       87.12         9.59\n",
      "312  2021.0        TOLIMA       5.03                4.42                  6.22             4.14          101.13                    98.01                      88.41                 53.61       87.43         7.54\n",
      "336  2022.0    LA GUAJIRA       3.65                3.44                  4.11             2.91           99.25                   100.13                      60.36                 28.43       86.54         9.81\n",
      "379  2024.0    LA GUAJIRA       2.80                2.64                  3.21             2.32          100.12                   102.76                      61.87                 30.33       88.68         8.51\n",
      "401  2016.0      CASANARE       4.21                3.59                  5.62             2.70          100.18                    99.15                      82.20                 47.43       90.23         5.56\n",
      "415  2022.0       BOLIVAR       4.01                3.61                  4.50             3.56          100.19                    97.69                      82.89                 51.06       88.82         7.17\n",
      "416  2013.0      CASANARE       3.67                3.14                  4.50             2.93          103.69                      NaN                      84.85                 45.46       90.86         5.47\n",
      "428  2016.0     ATLANTICO       2.13                2.07                  2.28             1.50           89.35                    85.78                      75.45                113.09       94.42          NaN\n",
      "444  2012.0      CASANARE       6.52                5.44                  8.39             6.08          103.88                   102.00                      85.40                 44.03       87.67         5.81\n",
      "449  2011.0         SUCRE       6.74                5.50                  7.25             8.18             NaN                   101.38                      76.16                 40.46       86.31         7.10\n",
      "455  2022.0        TOLIMA       4.71                3.94                  5.94             3.83          100.26                    98.62                      85.87                 53.68         NaN         6.90\n",
      "458  2021.0    LA GUAJIRA       2.06                2.06                  2.23             1.62           99.35                   100.04                      60.89                 27.93       89.12         8.82\n"
     ]
    }
   ],
   "source": [
    "# Find rows with any negative percentage value\n",
    "mask_negative = (df[percentage_cols] < 0).any(axis=1)\n",
    "mask_over_100 = (df[percentage_cols] > 100).any(axis=1)\n",
    "\n",
    "print(f\"Rows with negative values: {mask_negative.sum()}\")\n",
    "if mask_negative.sum() > 0:\n",
    "    print(df.loc[mask_negative, ['ano', 'departamento'] + percentage_cols].to_string())\n",
    "\n",
    "print(f\"\\nRows with values > 100: {mask_over_100.sum()}\")\n",
    "if mask_over_100.sum() > 0:\n",
    "    print(df.loc[mask_over_100, ['ano', 'departamento'] + percentage_cols].to_string())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.4 Fix Invalid Values\n",
    "\n",
    "We will replace invalid values with NaN, then fill them with the column median (the same strategy we used for missing values). This is the safer approach: we treat impossible values the same as missing data.\n",
    "\n",
    "**Run the cell below.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "desercion_primaria: fixed 1 invalid values (replaced with median 3.13)\n",
      "desercion_secundaria: fixed 2 invalid values (replaced with median 4.75)\n",
      "desercion_media: fixed 3 invalid values (replaced with median 3.28)\n",
      "cobertura_neta: fixed 12 invalid values (replaced with median 87.50)\n",
      "cobertura_neta_primaria: fixed 9 invalid values (replaced with median 85.96)\n",
      "cobertura_neta_secundaria: fixed 1 invalid values (replaced with median 72.73)\n",
      "cobertura_neta_media: fixed 1 invalid values (replaced with median 41.20)\n",
      "reprobacion: fixed 3 invalid values (replaced with median 6.26)\n",
      "\n",
      "Total invalid values fixed: 32\n"
     ]
    }
   ],
   "source": [
    "import numpy as np  # <<--- importante\n",
    "import pandas as pd\n",
    "\n",
    "total_fixed = 0\n",
    "\n",
    "for col in percentage_cols:\n",
    "    # Contar valores inválidos (<0 o >100)\n",
    "    invalid_mask = (df[col] < 0) | (df[col] > 100)\n",
    "    n_invalid = invalid_mask.sum()\n",
    "    \n",
    "    if n_invalid > 0:\n",
    "        # Reemplazar inválidos con NaN\n",
    "        df.loc[invalid_mask, col] = np.nan\n",
    "        \n",
    "        # Rellenar NaN con la mediana\n",
    "        median_val = df[col].median()\n",
    "        df[col] = df[col].fillna(median_val)\n",
    "        \n",
    "        print(f\"{col}: fixed {n_invalid} invalid values (replaced with median {median_val:.2f})\")\n",
    "        total_fixed += n_invalid\n",
    "\n",
    "print(f\"\\nTotal invalid values fixed: {total_fixed}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**What just happened:** We found values that were mathematically present but logically impossible (negative percentages, percentages above 100). We replaced them with NaN and then filled with the median, just like we do with missing values.\n",
    "\n",
    "**Key takeaway:** Domain knowledge is essential for data cleaning. Without knowing that dropout rates must be between 0 and 100, we would never catch these errors. The `describe()` function is your friend here: always check `min` and `max` against what you know about the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.5 Verify the Fix\n",
    "\n",
    "Let's confirm that all percentage values are now within the valid 0-100 range.\n",
    "\n",
    "**Run the cell below.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After fix - Min and Max for percentage columns:\n",
      "     desercion  desercion_primaria  desercion_secundaria  desercion_media  cobertura_neta  cobertura_neta_primaria  cobertura_neta_secundaria  cobertura_neta_media  aprobacion  reprobacion\n",
      "min       0.46                0.34                  0.38             0.42           50.79                    51.12                      24.06                  6.79       72.00          0.0\n",
      "max      10.94               10.73                 13.18            29.89           99.52                    99.47                      91.09                 65.32       99.08         17.6\n",
      "\n",
      "Invalid values remaining: 0\n"
     ]
    }
   ],
   "source": [
    "print(\"After fix - Min and Max for percentage columns:\")\n",
    "print(df[percentage_cols].describe().loc[['min', 'max']].round(2).to_string())\n",
    "\n",
    "remaining_invalid = (df[percentage_cols].lt(0).sum().sum() + df[percentage_cols].gt(100).sum().sum())\n",
    "print(f\"\\nInvalid values remaining: {remaining_invalid}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### QUESTION 4\n",
    "\n",
    "Which approach is better for invalid percentage values:\n",
    "\n",
    "**(a)** Replace with NaN and fill with the median (what we did), or  \n",
    "df[percentage_cols].describe().loc[['min', 'max']]\n",
    "\n",
    "**(b)** Clip to the valid range (set negatives to 0 and values >100 to 100)?\n",
    "\n",
    "remaining_invalid = (df[percentage_cols].lt(0).sum().sum() + df[percentage_cols].gt(100).sum().sum())\n",
    "Think about what each approach assumes. When might clipping be better? When might the median approach be better?\n",
    "\n",
    "*Double-click this cell and write your answer below:*\n",
    "\n",
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "13da7d2e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>desercion</th>\n",
       "      <th>desercion_primaria</th>\n",
       "      <th>desercion_secundaria</th>\n",
       "      <th>desercion_media</th>\n",
       "      <th>cobertura_neta</th>\n",
       "      <th>cobertura_neta_primaria</th>\n",
       "      <th>cobertura_neta_secundaria</th>\n",
       "      <th>cobertura_neta_media</th>\n",
       "      <th>aprobacion</th>\n",
       "      <th>reprobacion</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.46</td>\n",
       "      <td>0.34</td>\n",
       "      <td>0.38</td>\n",
       "      <td>0.42</td>\n",
       "      <td>50.79</td>\n",
       "      <td>51.12</td>\n",
       "      <td>24.06</td>\n",
       "      <td>6.79</td>\n",
       "      <td>72.00</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>10.94</td>\n",
       "      <td>10.73</td>\n",
       "      <td>13.18</td>\n",
       "      <td>29.89</td>\n",
       "      <td>99.52</td>\n",
       "      <td>99.47</td>\n",
       "      <td>91.09</td>\n",
       "      <td>65.32</td>\n",
       "      <td>99.08</td>\n",
       "      <td>17.6</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     desercion  desercion_primaria  desercion_secundaria  desercion_media  \\\n",
       "min       0.46                0.34                  0.38             0.42   \n",
       "max      10.94               10.73                 13.18            29.89   \n",
       "\n",
       "     cobertura_neta  cobertura_neta_primaria  cobertura_neta_secundaria  \\\n",
       "min           50.79                    51.12                      24.06   \n",
       "max           99.52                    99.47                      91.09   \n",
       "\n",
       "     cobertura_neta_media  aprobacion  reprobacion  \n",
       "min                  6.79       72.00          0.0  \n",
       "max                 65.32       99.08         17.6  "
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[percentage_cols].describe().loc[['min', 'max']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "2ba1be80",
   "metadata": {},
   "outputs": [],
   "source": [
    "remaining_invalid = (df[percentage_cols].lt(0).sum().sum() + df[percentage_cols].gt(100).sum().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Section 7: Summary\n",
    "\n",
    "We have completed all 5 cleaning steps. Let's compare the original dataset with our cleaned version to see the full impact of our work.\n",
    "\n",
    "**Run the cell below.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=======================================================\n",
      "  BEFORE CLEANING (original)\n",
      "=======================================================\n",
      "  Rows:                466\n",
      "  Total NaN:           616\n",
      "  Duplicates:          0\n",
      "  Unique departments:  34\n",
      "  ano dtype:           float64\n",
      "  poblacion dtype:     str\n",
      "\n",
      "=======================================================\n",
      "  AFTER CLEANING\n",
      "=======================================================\n",
      "  Rows:                466\n",
      "  Total NaN:           616\n",
      "  Duplicates:          0\n",
      "  Unique departments:  34\n",
      "  ano dtype:           float64\n",
      "  poblacion dtype:     str\n",
      "\n",
      "=======================================================\n",
      "  INVALID VALUES CHECK\n",
      "=======================================================\n",
      "  desercion: min=0.46, max=10.94\n",
      "  cobertura_neta: min=50.79, max=99.52\n",
      "  aprobacion: min=72.00, max=99.08\n",
      "  reprobacion: min=0.00, max=17.60\n"
     ]
    }
   ],
   "source": [
    "# Guardar copia del dataframe original antes de limpiar\n",
    "\n",
    "print(\"=\" * 55)\n",
    "print(\"  BEFORE CLEANING (original)\")\n",
    "print(\"=\" * 55)\n",
    "print(f\"  Rows:                {len(df_original)}\")\n",
    "print(f\"  Total NaN:           {df_original.isnull().sum().sum()}\")\n",
    "print(f\"  Duplicates:          {df_original.duplicated().sum()}\")\n",
    "print(f\"  Unique departments:  {df_original['departamento'].nunique()}\")\n",
    "print(f\"  ano dtype:           {df_original['ano'].dtype}\")\n",
    "print(f\"  poblacion dtype:     {df_original['poblacion_5_16'].dtype}\")\n",
    "\n",
    "print()\n",
    "\n",
    "print(\"=\" * 55)\n",
    "print(\"  AFTER CLEANING\")\n",
    "print(\"=\" * 55)\n",
    "print(f\"  Rows:                {len(df)}\")\n",
    "print(f\"  Total NaN:           {df.isnull().sum().sum()}\")\n",
    "print(f\"  Duplicates:          {df.duplicated().sum()}\")\n",
    "print(f\"  Unique departments:  {df['departamento'].nunique()}\")\n",
    "print(f\"  ano dtype:           {df['ano'].dtype}\")\n",
    "print(f\"  poblacion dtype:     {df['poblacion_5_16'].dtype}\")\n",
    "\n",
    "print()\n",
    "print(\"=\" * 55)\n",
    "print(\"  INVALID VALUES CHECK\")\n",
    "print(\"=\" * 55)\n",
    "pct_cols_check = ['desercion', 'cobertura_neta', 'aprobacion', 'reprobacion']\n",
    "for col in pct_cols_check:\n",
    "    print(f\"  {col}: min={df[col].min():.2f}, max={df[col].max():.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The 5-Step Cleaning Workflow\n",
    "\n",
    "Here is the workflow you just practiced. Use these exact steps in every data project:\n",
    "\n",
    "| Step | What | Key Commands |\n",
    "|------|------|--------------|\n",
    "| 1. **Inspect** | Understand the data before touching it | `df.shape`, `df.dtypes`, `df.isnull().sum()`, `df.describe()` |\n",
    "| 2. **Handle missing** | Decide: drop, fill with 0, or fill with median | `fillna(0)`, `fillna(median)`, `dropna(subset=...)` |\n",
    "| 3. **Fix types** | Numbers stored as text, floats that should be int | `pd.to_numeric(errors='coerce')`, `astype(int)` |\n",
    "| 4. **Remove duplicates** | Exact copies that inflate counts | `duplicated().sum()`, `drop_duplicates()` |\n",
    "| 5. **Validate values** | Impossible values based on domain knowledge | `describe()` min/max, boolean masks |\n",
    "\n",
    "**Remember:** Cleaning is iterative. Fixing one problem can create another (like text standardization creating new duplicates). Always verify after each step.\n",
    "\n",
    "This dataset is now ready for analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FINAL REFLECTION\n",
    "\n",
    "Write 2-3 sentences:\n",
    "\n",
    "1. What was the most surprising thing you found during cleaning?\n",
    "2. Which of the 5 steps do you think is most important? Why?\n",
    "3. How would you apply these steps to your project dataset from datos.gov.co?\n",
    "\n",
    "Lo más sorprendente fue encontrar valores inválidos, como porcentajes negativos o mayores a 100, que no se detectan automáticamente y podrían distorsionar cualquier análisis. Considero que el paso más importante es validar los valores, porque incluso si no hay datos faltantes ni duplicados, el dataset puede contener errores lógicos o imposibles\n",
    "\n",
    "..."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
